# -*- coding: utf-8 -*-
"""Car-Object-Dection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rz_KCzqYRyhczrLulhXU0TK02Lo2pxpR
"""

# Install Ultralytics (YOLOv8) and OpenCV
!pip install ultralytics opencv-python-headless
!pip install ultralytics opencv-python-headless filterpy

import cv2
import torch
from ultralytics import YOLO
import os
from IPython.display import display, HTML
from base64 import b64encode

from google.colab import files

uploaded = files.upload()
# Upload fille



from ultralytics import YOLO

# Load pre-trained YOLOv8n (you can replace with yolov8s.pt or your custom .pt model)
model = YOLO("yolov8n.pt")

# Define target classes and their colors (BGR)
target_classes = {
    2: "Car",
    5: "Bus",
    7: "Truck"
}

class_colors = {
    "Car": (0, 255, 0),
    "Bus": (255, 0, 0),
    "Truck": (0, 0, 255)
}

# Simple tracker (each detection gets a unique ID)
class Tracker:
    def __init__(self):
        self.track_id = 0

    def update(self, detections):
        tracked_objects = []
        for det in detections:
            x1, y1, x2, y2, cls_id = det
            tracked_objects.append([x1, y1, x2, y2, self.track_id, cls_id])
            self.track_id += 1
        return tracked_objects

tracker = Tracker()

import cv2

# Input video path (uploaded in Cell 2)
video_path = list(uploaded.keys())[0]
cap = cv2.VideoCapture(video_path)

# Video properties
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

# Output writer
output_path = "vehicles_tracked_output.mp4"
fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)[0]
    detections = []

    for box in results.boxes:
        cls_id = int(box.cls[0])
        conf = float(box.conf[0])

        if cls_id in target_classes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            detections.append([x1, y1, x2, y2, cls_id])

    tracked = tracker.update(detections)

    for trk in tracked:
        x1, y1, x2, y2, track_id, cls_id = trk
        label = target_classes.get(cls_id, "Object")
        color = class_colors.get(label, (255, 255, 255))

        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        text = f"{label} #{track_id}"
        cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    out.write(frame)

    frame_count += 1
    if frame_count % 30 == 0:
        print(f"Processed {frame_count} frames...")

cap.release()
out.release()
print(f"âœ… Done. Output saved to {output_path}")

from IPython.display import HTML
from base64 import b64encode

def display_video(path):
    mp4 = open(path, 'rb').read()
    data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
    return HTML(f'<video width=640 controls><source src="{data_url}" type="video/mp4"></video>')

display_video("vehicles_tracked_output.mp4")